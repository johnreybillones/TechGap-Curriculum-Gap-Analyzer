# -*- coding: utf-8 -*-
"""ML Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/181svDmGrjb0wi9tU3BRbaNHv80GO9F-S
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sentence_transformers import SentenceTransformer
import random
import joblib
import gc

# --- METRICS IMPORTS ---
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
)

# --- 1. Configuration ---
scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Running on device: {DEVICE}")

try:
    sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')
    print("SBERT model loaded.")
except Exception as e:
    sbert_model = None
    print(f"Error loading SBERT: {e}")

# --- 2. Siamese Network ---
class SiameseNetwork(nn.Module):
    def __init__(self, input_dim=384, output_dim=128):
        super(SiameseNetwork, self).__init__()
        self.projector = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, output_dim)
        )

    def forward_one(self, x):
        return self.projector(x)

    def forward(self, x1, x2):
        out1 = self.forward_one(x1)
        out2 = self.forward_one(x2)
        return out1, out2

# --- 3. Dataset & Helpers ---
class SiameseJobDataset(Dataset):
    def __init__(self, job_data, sbert_model):
        self.job_data = job_data
        self.sbert_model = sbert_model
        self.data_by_title = {}
        for item in job_data:
            t = item['title']
            if t not in self.data_by_title: self.data_by_title[t] = []
            self.data_by_title[t].append(item['text'])
        self.titles = list(self.data_by_title.keys())

    def __len__(self): return 1000

    def __getitem__(self, idx):
        is_positive = random.random() > 0.5
        if is_positive:
            valid_titles = [t for t in self.titles if len(self.data_by_title[t]) > 1]
            if not valid_titles: return self.__getitem__(idx)
            title = random.choice(valid_titles)
            text1, text2 = random.sample(self.data_by_title[title], 2)
            label = 1.0
        else:
            title1, title2 = random.sample(self.titles, 2)
            text1 = random.choice(self.data_by_title[title1])
            text2 = random.choice(self.data_by_title[title2])
            label = -1.0

        emb1 = self.sbert_model.encode(text1, convert_to_tensor=True, show_progress_bar=False, device='cpu')
        emb2 = self.sbert_model.encode(text2, convert_to_tensor=True, show_progress_bar=False, device='cpu')
        return emb1, emb2, torch.tensor(label, dtype=torch.float)

def load_curriculum_profiles(filepath, c_type):
    try:
        df = pd.read_csv(filepath, on_bad_lines='skip', engine='python', usecols=['Course Description', 'Outcome Text'])
        texts = []
        for _, row in df.iterrows():
            txt = ' '.join(row.dropna().astype(str))
            if txt.strip(): texts.append(txt)
        del df
        gc.collect()
        return {c_type: texts}
    except Exception as e:
        return {}

def load_job_roles(filepath):
    try:
        df = pd.read_csv(filepath)
        jobs = []
        desc_cols = [c for c in df.columns if 'description' in c.lower() or 'skill' in c.lower()]
        title_cols = [c for c in df.columns if 'title' in c.lower() or 'role' in c.lower()]

        for _, row in df.iterrows():
            title = str(row[title_cols[0]]).strip()
            desc = " ".join([str(row[c]) for c in desc_cols if pd.notna(row[c])])
            if title and desc: jobs.append({'title': title, 'text': desc})

        del df # Free memory
        gc.collect()
        return jobs
    except Exception as e:
        print(f"Error loading jobs: {e}")
        return []

# --- 4. Training Functions ---

def train_siamese(job_data, sbert_model):
    print("\n--- Training Siamese Network ---")
    dataset = SiameseJobDataset(job_data, sbert_model)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0)

    model = SiameseNetwork().to(DEVICE)
    model.train()
    criterion = nn.CosineEmbeddingLoss(margin=0.5)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(5):
        total_loss = 0
        for emb1, emb2, labels in dataloader:
            emb1, emb2, labels = emb1.to(DEVICE), emb2.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()

            if scaler:
                with torch.cuda.amp.autocast():
                    out1, out2 = model(emb1, emb2)
                    loss = criterion(out1, out2, labels)
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                out1, out2 = model(emb1, emb2)
                loss = criterion(out1, out2, labels)
                loss.backward()
                optimizer.step()

            total_loss += loss.item()

        # Clean up GPU cache after every epoch
        if torch.cuda.is_available(): torch.cuda.empty_cache()
        print(f"Siamese Epoch {epoch+1}: Loss {total_loss/len(dataloader):.4f}")

    return model

def train_and_evaluate_naive_bayes(job_data):
    print("\n--- Training & Evaluating Naive Bayes Classifier ---")
    texts = [j['text'] for j in job_data]
    labels = [j['title'] for j in job_data]

    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)

    nb_model = make_pipeline(TfidfVectorizer(max_features=5000), MultinomialNB()) # Limit features to save RAM
    nb_model.fit(X_train, y_train)
    y_pred = nb_model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    cm = confusion_matrix(y_test, y_pred)

    # 5. Print Metrics
    print("\n" + "="*40)
    print("NAIVE BAYES MODEL PERFORMANCE")
    print("="*40)
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall:    {rec:.4f}")
    print(f"F1-Score:  {f1:.4f}")
    print("-" * 40)
    print("Confusion Matrix (Rows=True, Cols=Pred):")
    print(cm)
    print("-" * 40)

    # Retrain on full
    final_model = make_pipeline(TfidfVectorizer(max_features=5000), MultinomialNB())
    final_model.fit(texts, labels)
    return final_model

# --- 5. Main Execution ---
if __name__ == "__main__":
    if sbert_model:
        curriculum_files = {
            "CS Game Dev": "CS GAME DEV CURRICULUM.csv",
            "CS Intelligent Systems": "CS IS CURRICULUM.csv",
            "IT Network": "IT Network Curriculum.csv",
            "IT Web Dev": "IT Web Dev Curriculum.csv"
        }
        JOB_FILE = "/content/JobsDataset.csv"

        job_data = load_job_roles(JOB_FILE)

        if job_data:
            # --- A. TRAIN ---
            siamese_net = train_siamese(job_data, sbert_model)
            siamese_net.eval()

            # Clear Training Memory
            if torch.cuda.is_available(): torch.cuda.empty_cache()
            gc.collect()

            nb_classifier = train_and_evaluate_naive_bayes(job_data)

            # --- B. HYBRID ANALYSIS (Optimized) ---
            print("\n--- Running Hybrid Analysis ---")

            # Optimization: Pre-compute job vectors in batches and store on CPU
            job_vectors = []

            # Use SBERT on GPU for speed, then move result to CPU
            sbert_model.to(DEVICE)

            all_job_texts = [j['text'] for j in job_data]

            # Batch encode jobs to save RAM
            with torch.no_grad():
                # Encode in batches of 32
                raw_embs = sbert_model.encode(all_job_texts, batch_size=32, convert_to_tensor=True, show_progress_bar=True, device=DEVICE)

                # Pass through Siamese projector in batches
                processed_embs = []
                for i in range(0, len(raw_embs), 32):
                    batch = raw_embs[i:i+32]
                    # Project and immediately move to CPU to free VRAM
                    proj = siamese_net.forward_one(batch).cpu()
                    processed_embs.append(proj)

                # Flatten list
                final_job_embs = torch.cat(processed_embs)

                # Store lightweight references
                for i, job in enumerate(job_data):
                    job_vectors.append({'title': job['title'], 'vector': final_job_embs[i]})

            # Clear heavy raw embeddings
            del raw_embs
            del processed_embs
            del final_job_embs
            torch.cuda.empty_cache()

            results = []
            for profile_name, filename in curriculum_files.items():
                curr_data = load_curriculum_profiles(filename, profile_name)
                curr_texts = curr_data.get(profile_name, [])

                if not curr_texts: continue

                # NB Predict
                nb_prediction = nb_classifier.predict([" ".join(curr_texts)])[0]

                # Siamese Alignment (Batch processing)
                with torch.no_grad():
                    # Encode current curriculum (Batch size limits RAM usage)
                    raw_curr = sbert_model.encode(curr_texts, batch_size=32, convert_to_tensor=True, device=DEVICE)

                    # Project
                    curr_proj = siamese_net.forward_one(raw_curr)

                    # Mean and move to CPU
                    curr_vec = torch.mean(curr_proj, dim=0).cpu()

                # Calculate Similarity on CPU (System RAM is usually larger than VRAM)
                scores = []
                for job in job_vectors:
                    # Both vectors are on CPU now
                    sim = F.cosine_similarity(curr_vec.unsqueeze(0), job['vector'].unsqueeze(0))
                    scores.append({'title': job['title'], 'score': sim.item()})

                top_matches = sorted(scores, key=lambda x: x['score'], reverse=True)[:5]
                avg_alignment = np.mean([x['score'] for x in top_matches])

                results.append({
                    "Curriculum": profile_name,
                    "NB Predicted": nb_prediction,
                    "Siamese Alignment": f"{avg_alignment*100:.1f}%",
                    "Top Jobs": ", ".join([m['title'] for m in top_matches])
                })

            df_results = pd.DataFrame(results)
            print("\n" + df_results.to_string())

            # Save
            torch.save(siamese_net.state_dict(), "techgap_siamese.pth")
            joblib.dump(nb_classifier, "techgap_nb.pkl")
            print("\nModels saved.")
        else:
            print("No job data found.")

